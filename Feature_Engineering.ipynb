{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sujitkumar205/RICEVD/blob/main/Feature_Selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1591,
   "metadata": {
    "id": "SIX5WufmAr99"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_extraction.text  import CountVectorizer\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "from scipy.spatial import distance\n",
    "import statistics\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from scipy.stats import gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1592,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "start_time = datetime.now()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yj6wCN2r8J88"
   },
   "source": [
    "# ReliefF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1593,
   "metadata": {
    "id": "mzlhJsIlXT9D"
   },
   "outputs": [],
   "source": [
    "def reliefF(df, number_of_neighbours, instances_to_select, number_of_features):\n",
    "  features = df.iloc[:,:-1]\n",
    "  labels = df.iloc[:,-1]\n",
    "  rows,columns = features.shape\n",
    "\n",
    "  #initialize weights to zero\n",
    "  weights = np.zeros(columns,dtype = 'int')\n",
    "\n",
    "  #unique labels\n",
    "  unique_labels = np.unique(labels)\n",
    "\n",
    "  #used to select random instance\n",
    "  instances=np.array(list(range(1,rows)))\n",
    "\n",
    "  #difference between maximum and minimum of each feature used to calculate diff\n",
    "  minimums=np.min(features.values,axis=0)\n",
    "  maximums=np.max(features.values,axis=0)\n",
    "  difference=np.subtract(maximums,minimums)\n",
    "\n",
    "  for i in range(instances_to_select):\n",
    "\n",
    "    #choose a random instance and remove from instances to avoid selecting same thing again\n",
    "    random_instance = np.random.choice(instances[:-1])\n",
    "    instances=np.delete(instances,np.where(instances==random_instance))\n",
    "\n",
    "    #features of random instance used to calculate diff later on\n",
    "    random_instance_features = features.iloc[random_instance,:].values\n",
    "\n",
    "    #label of random instance and probability of label class\n",
    "    random_instance_label = labels[random_instance]\n",
    "    probability_random_instance_label = len(np.where(labels==random_instance_label)[0])/rows\n",
    "\n",
    "    #calculate euclidean distance between random instance and all other instances\n",
    "    distances = []\n",
    "    for temp in instances:\n",
    "      temp_features = features.iloc[temp,:].values\n",
    "      dist = distance.euclidean(random_instance_features,temp_features)\n",
    "      distances.append(dist)\n",
    "    \n",
    "    #sort instances based on distances\n",
    "    distances = np.array(distances)\n",
    "    arr1inds = distances.argsort()\n",
    "    sorted_distances = distances[arr1inds[::]]\n",
    "    sorted_instances = instances[arr1inds[::]]\n",
    "\n",
    "    #initialize list of nearest hits for random instance label and dictionary of nearest misses for every other label\n",
    "    nearest_hits = []\n",
    "    nearest_misses = {}\n",
    "\n",
    "    #finding nearest hits for random instance label\n",
    "    for temp in sorted_instances:\n",
    "      if labels[temp] == random_instance_label:\n",
    "        nearest_hits.append(temp)\n",
    "      if len(nearest_hits) == number_of_neighbours:\n",
    "        break\n",
    "      \n",
    "    #finding nearest misses for all other labels\n",
    "    for x in unique_labels:\n",
    "      if x == random_instance_label:\n",
    "        continue      \n",
    "      nearest_misses[x] = []\n",
    "      for temp in sorted_instances:\n",
    "        if labels[temp] == x:\n",
    "          nearest_misses[x].append(temp)\n",
    "        if len(nearest_misses[x]) == number_of_neighbours:\n",
    "          break\n",
    "\n",
    "    #used to find sum of diff function in weights equation for hits\n",
    "    total_hit = np.zeros(columns,dtype='int')\n",
    "\n",
    "    #find sum of diff function in weights equation for hits\n",
    "    for hit in range(len(nearest_hits)):\n",
    "      hI = features.iloc[nearest_hits[hit],:].values\n",
    "      dRH = np.divide(np.abs(np.subtract(random_instance_features,hI)),difference)\n",
    "      dRH = dRH/(instances_to_select * number_of_neighbours)\n",
    "      total_hit = np.add(total_hit,dRH)\n",
    "\n",
    "    #used to find sum of diff function in weights equation for misses\n",
    "    total_miss=np.zeros(columns,dtype='int')\n",
    "\n",
    "    #find sum of diff function in weights equation for misses in each class\n",
    "    for each_label in nearest_misses:\n",
    "      temp_miss=np.zeros(columns,dtype='int')\n",
    "      pclass=len(np.where(labels==each_label)[0])/rows #getting the probability of getting this class\n",
    "      postProb=pclass/(1-probability_random_instance_label) #calculating the posterior probability of getting this class\n",
    "\n",
    "      for each_miss in nearest_misses[each_label]:\n",
    "        mI = features.iloc[each_miss,:].values\n",
    "        dRM = np.divide(np.abs(np.subtract(random_instance_features,mI)),difference)\n",
    "        dRM = dRM/(instances_to_select * number_of_neighbours)\n",
    "        temp_miss = np.add(temp_miss,dRM)\n",
    "\n",
    "      total_miss = np.add(total_miss,(temp_miss*postProb))\n",
    "    \n",
    "    #update value of weights based on total hits and total miss and diff function values\n",
    "    weights=np.add(weights,total_miss)\n",
    "    weights=np.subtract(weights,total_hit) \n",
    "    \n",
    "\n",
    "  #select number_of_features weights with highest values and sort\n",
    "  ind = np.argpartition(weights, -number_of_features)[-number_of_features:]\n",
    "  ind = np.sort(ind)[::-1]\n",
    "\n",
    "  #column names of data frame\n",
    "  feature_names = list(df.columns.values)\n",
    "  feature_names = np.array(feature_names)\n",
    "\n",
    "  #top features based on weights\n",
    "  top_features = feature_names[ind]\n",
    "\n",
    "  return top_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FI8bHk_GAhEg"
   },
   "source": [
    "\n",
    "# Chi-Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1594,
   "metadata": {
    "id": "VQvpZlenH-eQ"
   },
   "outputs": [],
   "source": [
    "def chi_square(df, number_of_features):\n",
    "  features = df.iloc[:,:-1]\n",
    "  labels = df.iloc[:,-1]\n",
    "  labels=labels.astype('int') \n",
    "  test = SelectKBest(score_func=chi2, k=number_of_features)\n",
    "  fit = test.fit(features, labels)\n",
    "  chisquare_features = fit.get_feature_names_out(input_features=None)\n",
    "  return chisquare_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lvGvnBFX2-m"
   },
   "source": [
    "# SVM-RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1595,
   "metadata": {
    "id": "Nc5vrniyIvva"
   },
   "outputs": [],
   "source": [
    "def svmrfe(df, number_of_features):\n",
    "  features = df.iloc[:,:-1]\n",
    "  labels = df.iloc[:,-1]\n",
    "  #estimator = SVR(kernel=\"linear\",cache_size=7000)\n",
    "  #estimator = LinearSVR(max_iter=100000,dual = True)\n",
    "  estimator = LinearSVC(random_state=0, tol=1e-5)\n",
    "  selector = RFE(estimator, n_features_to_select=number_of_features, step=10)\n",
    "  selector = selector.fit(features, labels)\n",
    "  feature_ranks = list(selector.ranking_)\n",
    "  feature_names = list(df.columns.values)\n",
    "  rank_dictionary = dict(zip(feature_names, feature_ranks))\n",
    "  svmrfe_features = [feature for feature, rank in rank_dictionary.items() if rank == 1]\n",
    "  return svmrfe_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Adpmy9YZchUR"
   },
   "source": [
    "# ReliefF Variable Distance\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/spatial.distance.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1596,
   "metadata": {
    "id": "u8pkVtxAnO9s"
   },
   "outputs": [],
   "source": [
    "def calcDistance(random_instance_features, temp_features, distance_variable):\n",
    "  # print(\"inside the calc distance function\")\n",
    "  # print(\"distance variable is\") \n",
    "  # print(distance_variable)\n",
    "  if distance_variable == 0:\n",
    "    dist = distance.braycurtis(random_instance_features,temp_features)\n",
    "    return dist\n",
    "  if distance_variable == 1:\n",
    "    dist = distance.canberra(random_instance_features,temp_features)\n",
    "    return dist\n",
    "  if distance_variable == 2:\n",
    "    dist = distance.chebyshev(random_instance_features,temp_features)\n",
    "    return dist\n",
    "  if distance_variable == 3:\n",
    "    dist = distance.cityblock(random_instance_features,temp_features)\n",
    "    return dist\n",
    "  if distance_variable == 4:\n",
    "    dist = distance.correlation(random_instance_features,temp_features)\n",
    "    return dist\n",
    "  if distance_variable == 5:\n",
    "    dist = distance.cosine(random_instance_features,temp_features)\n",
    "    return dist\n",
    "  if distance_variable == 6:\n",
    "    dist = distance.euclidean(random_instance_features,temp_features)\n",
    "    return dist\n",
    "  if distance_variable == 7:\n",
    "    dist = distance.jensenshannon(random_instance_features,temp_features)\n",
    "    return dist\n",
    "  if distance_variable == 8:\n",
    "    dist = distance.sqeuclidean(random_instance_features,temp_features)\n",
    "    return dist\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1597,
   "metadata": {
    "id": "6mbgNO1WchUY"
   },
   "outputs": [],
   "source": [
    "# def reliefF_variable(df, number_of_neighbours, instances_to_select, number_of_features, distance_variable):\n",
    "#   print(\"inside the reliefF_variable func\")\n",
    "\n",
    "#   features = df.iloc[:,:-1]\n",
    "#   labels = df.iloc[:,-1]\n",
    "#   rows,columns = features.shape\n",
    "\n",
    "#   #initialize weights to zero\n",
    "#   # weights = np.zeros(columns,dtype = 'int')\n",
    "\n",
    "#   #unique labels\n",
    "#   unique_labels = np.unique(labels)\n",
    "\n",
    "#   #used to select random instance\n",
    "#   instances=np.array(list(range(1,rows)))\n",
    "\n",
    "#   #difference between maximum and minimum of each feature used to calculate diff\n",
    "#   minimums=np.min(features.values,axis=0)\n",
    "#   maximums=np.max(features.values,axis=0)\n",
    "#   difference=np.subtract(maximums,minimums)\n",
    "  \n",
    "#   total_instances = instances_to_select * len(unique_labels)\n",
    "#   label_count = {}\n",
    "\n",
    "#   for i in unique_labels:\n",
    "#     label_count[i] = 0\n",
    "\n",
    "#   for i in range(total_instances):\n",
    "\n",
    "#     #choose a random instance and remove from instances to avoid selecting same thing again\n",
    "#     random_instance = np.random.choice(instances[:-1])\n",
    "#     instances=np.delete(instances,np.where(instances==random_instance))\n",
    "\n",
    "#     #features of random instance used to calculate diff later on\n",
    "#     random_instance_features = features.iloc[random_instance,:].values\n",
    "\n",
    "#     #label of random instance and probability of label class\n",
    "#     random_instance_label = labels[random_instance]\n",
    "#     probability_random_instance_label = len(np.where(labels==random_instance_label)[0])/rows\n",
    "\n",
    "#     if label_count[random_instance_label] >= instances_to_select:\n",
    "#       i = i-1\n",
    "#       continue\n",
    "    \n",
    "#     else:\n",
    "#       label_count[random_instance_label] = label_count[random_instance_label] + 1\n",
    "\n",
    "#     #calculate euclidean distance between random instance and all other instances\n",
    "#     distances = []\n",
    "#     for temp in instances:\n",
    "#       temp_features = features.iloc[temp,:].values\n",
    "#       dist = calcDistance(random_instance_features, temp_features, distance_variable)\n",
    "#       distances.append(dist)\n",
    "    \n",
    "#     #sort instances based on distances\n",
    "#     distances = np.array(distances)\n",
    "#     arr1inds = distances.argsort()\n",
    "#     sorted_distances = distances[arr1inds[::]]\n",
    "#     sorted_instances = instances[arr1inds[::]]\n",
    "\n",
    "#     #initialize list of nearest hits for random instance label and dictionary of nearest misses for every other label\n",
    "#     nearest_hits = []\n",
    "#     nearest_misses = {}\n",
    "\n",
    "#     #finding nearest hits for random instance label\n",
    "#     for temp in sorted_instances:\n",
    "#       if labels[temp] == random_instance_label:\n",
    "#         nearest_hits.append(temp)\n",
    "#       if len(nearest_hits) == number_of_neighbours:\n",
    "#         break\n",
    "      \n",
    "#     #finding nearest misses for all other labels\n",
    "#     for x in unique_labels:\n",
    "#       if x == random_instance_label:\n",
    "#         continue      \n",
    "#       nearest_misses[x] = []\n",
    "#       for temp in sorted_instances:\n",
    "#         if labels[temp] == x:\n",
    "#           nearest_misses[x].append(temp)\n",
    "#         if len(nearest_misses[x]) == number_of_neighbours:\n",
    "#           break\n",
    "\n",
    "#     #used to find sum of diff function in weights equation for hits\n",
    "#     total_hit = np.zeros(columns,dtype='int')\n",
    "\n",
    "#     #find sum of diff function in weights equation for hits\n",
    "#     for hit in range(len(nearest_hits)):\n",
    "#       hI = features.iloc[nearest_hits[hit],:].values\n",
    "#       dRH = np.divide(np.abs(np.subtract(random_instance_features,hI)),difference)\n",
    "#       dRH = dRH/(instances_to_select * number_of_neighbours)\n",
    "#       total_hit = np.add(total_hit,dRH)\n",
    "\n",
    "#     #used to find sum of diff function in weights equation for misses\n",
    "#     total_miss=np.zeros(columns,dtype='int')\n",
    "\n",
    "#     #find sum of diff function in weights equation for misses in each class\n",
    "#     for each_label in nearest_misses:\n",
    "#       temp_miss=np.zeros(columns,dtype='int')\n",
    "#       pclass=len(np.where(labels==each_label)[0])/rows #getting the probability of getting this class\n",
    "#       postProb=pclass/(1-probability_random_instance_label) #calculating the posterior probability of getting this class\n",
    "\n",
    "#       for each_miss in nearest_misses[each_label]:\n",
    "#         mI = features.iloc[each_miss,:].values\n",
    "#         dRM = np.divide(np.abs(np.subtract(random_instance_features,mI)),difference)\n",
    "#         dRM = dRM/(instances_to_select * number_of_neighbours)\n",
    "#         temp_miss = np.add(temp_miss,dRM)\n",
    "\n",
    "#       total_miss = np.add(total_miss,(temp_miss*postProb))\n",
    "    \n",
    "#     #update value of weights based on total hits and total miss and diff function values\n",
    "#     weights=np.add(weights,total_miss)\n",
    "#     weights=np.subtract(weights,total_hit) \n",
    "    \n",
    "\n",
    "#   #select number_of_features weights with highest values and sort\n",
    "#   ind = np.argpartition(weights, -number_of_features)[-number_of_features:]\n",
    "#   ind = np.sort(ind)[::-1]\n",
    "\n",
    "#   #column names of data frame\n",
    "#   feature_names = list(df.columns.values)\n",
    "#   feature_names = np.array(feature_names)\n",
    "\n",
    "#   #top features based on weights\n",
    "#   top_features = feature_names[ind]\n",
    "\n",
    "#   return top_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1598,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reliefF_variable(df, number_of_neighbours, instances_to_select, number_of_features, distance_variable):\n",
    "  print(\"SSSSSSSSSSSSSSSSSSS\")\n",
    "  print(\"inside the reliefF_variable func2\")\n",
    "\n",
    "  features = df.iloc[:,:-1]\n",
    "  labels = df.iloc[:,-1]\n",
    "  rows,columns = features.shape\n",
    "\n",
    "  #initialize weights to zero\n",
    "  weights = np.zeros(columns,dtype = 'int')\n",
    "\n",
    "  #unique labels\n",
    "  unique_labels = np.unique(labels)\n",
    "\n",
    "  #used to select random instance\n",
    "  instances=np.array(list(range(1,rows)))\n",
    "\n",
    "  #difference between maximum and minimum of each feature used to calculate diff\n",
    "  minimums=np.min(features.values,axis=0)\n",
    "  maximums=np.max(features.values,axis=0)\n",
    "  difference=np.subtract(maximums,minimums)\n",
    "  \n",
    "  total_instances = instances_to_select * len(unique_labels)\n",
    "  label_count = {}\n",
    "\n",
    "  for i in unique_labels:\n",
    "    label_count[i] = 0\n",
    "\n",
    "  for i in range(total_instances):\n",
    "\n",
    "    #choose a random instance and remove from instances to avoid selecting same thing again\n",
    "    random_instance = np.random.choice(instances[:-1])\n",
    "    instances=np.delete(instances,np.where(instances==random_instance))\n",
    "\n",
    "    #features of random instance used to calculate diff later on\n",
    "    random_instance_features = features.iloc[random_instance,:].values\n",
    "\n",
    "    #label of random instance and probability of label class\n",
    "    random_instance_label = labels[random_instance]\n",
    "    probability_random_instance_label = len(np.where(labels==random_instance_label)[0])/rows\n",
    "\n",
    "    if label_count[random_instance_label] >= instances_to_select:\n",
    "      i = i-1\n",
    "      continue\n",
    "    \n",
    "    else:\n",
    "      label_count[random_instance_label] = label_count[random_instance_label] + 1\n",
    "\n",
    "    #calculate euclidean distance between random instance and all other instances\n",
    "    distances = []\n",
    "    for temp in instances:\n",
    "      temp_features = features.iloc[temp,:].values\n",
    "      dist = calcDistance(random_instance_features, temp_features, distance_variable)\n",
    "      distances.append(dist)\n",
    "    \n",
    "    #sort instances based on distances\n",
    "    distances = np.array(distances)\n",
    "    arr1inds = distances.argsort()\n",
    "    sorted_distances = distances[arr1inds[::]]\n",
    "    sorted_instances = instances[arr1inds[::]]\n",
    "\n",
    "    #initialize list of nearest hits for random instance label and dictionary of nearest misses for every other label\n",
    "    nearest_hits = []\n",
    "    nearest_misses = {}\n",
    "\n",
    "    #finding nearest hits for random instance label\n",
    "    for temp in sorted_instances:\n",
    "      if labels[temp] == random_instance_label:\n",
    "        nearest_hits.append(temp)\n",
    "      if len(nearest_hits) == number_of_neighbours:\n",
    "        break\n",
    "      \n",
    "    #finding nearest misses for all other labels\n",
    "    for x in unique_labels:\n",
    "      if x == random_instance_label:\n",
    "        continue      \n",
    "      nearest_misses[x] = []\n",
    "      for temp in sorted_instances:\n",
    "        if labels[temp] == x:\n",
    "          nearest_misses[x].append(temp)\n",
    "        if len(nearest_misses[x]) == number_of_neighbours:\n",
    "          break\n",
    "\n",
    "    #used to find sum of diff function in weights equation for hits\n",
    "    total_hit = np.zeros(columns,dtype='int')\n",
    "\n",
    "    #find sum of diff function in weights equation for hits\n",
    "    for hit in range(len(nearest_hits)):\n",
    "      hI = features.iloc[nearest_hits[hit],:].values\n",
    "      dRH = np.divide(np.abs(np.subtract(random_instance_features,hI)),difference)\n",
    "      dRH = dRH/(instances_to_select * number_of_neighbours)\n",
    "      total_hit = np.add(total_hit,dRH)\n",
    "\n",
    "    #used to find sum of diff function in weights equation for misses\n",
    "    total_miss=np.zeros(columns,dtype='int')\n",
    "\n",
    "    #find sum of diff function in weights equation for misses in each class\n",
    "    for each_label in nearest_misses:\n",
    "      temp_miss=np.zeros(columns,dtype='int')\n",
    "      pclass=len(np.where(labels==each_label)[0])/rows #getting the probability of getting this class\n",
    "      postProb=pclass/(1-probability_random_instance_label) #calculating the posterior probability of getting this class\n",
    "\n",
    "      for each_miss in nearest_misses[each_label]:\n",
    "        mI = features.iloc[each_miss,:].values\n",
    "        dRM = np.divide(np.abs(np.subtract(random_instance_features,mI)),difference)\n",
    "        dRM = dRM/(instances_to_select * number_of_neighbours)\n",
    "        temp_miss = np.add(temp_miss,dRM)\n",
    "\n",
    "      total_miss = np.add(total_miss,(temp_miss*postProb))\n",
    "    \n",
    "    #update value of weights based on total hits and total miss and diff function values\n",
    "    weights=np.add(weights,total_miss)\n",
    "    weights=np.subtract(weights,total_hit) \n",
    "    \n",
    "\n",
    "  #select number_of_features weights with highest values and sort\n",
    "  ind = np.argpartition(weights, -number_of_features)[-number_of_features:]\n",
    "  ind = np.sort(ind)[::-1]\n",
    "\n",
    "  #column names of data frame\n",
    "  feature_names = list(df.columns.values)\n",
    "  feature_names = np.array(feature_names)\n",
    "\n",
    "  #top features based on weights\n",
    "  top_features = feature_names[ind]\n",
    "\n",
    "  return top_features\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanced Sampling: Instead of random sampling, use stratified sampling to ensure that instances from both minority and majority classes are selected proportional to their representation in the dataset. This can help in giving equal importance to all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This implementation includes the following modifications for balanced sampling: \r",
    "#### Class Proportions Calculation: Determines how many instances to select from each class based on the class distribution within the dataset.\r",
    ".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance Selection Per Class: For each class, randomly selects instances up to the calculated limit, ensuring balanced representation.\n",
    "#### Distance Calculation and Nearest Neighbor Identification: Separately for hits (same class) and misses (different classes), ensuring that the algorithm correctly identifies relevant neighbors within the balanced sampling framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1599,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# def calcDistance(instance1, instance2):\n",
    "#     \"\"\"\n",
    "#     Calculate the Euclidean distance between two instances.\n",
    "#     \"\"\"\n",
    "#     return np.sqrt(np.sum((instance1 - instance2) ** 2))\n",
    "\n",
    "def reliefF_variable_balanced(df, number_of_neighbours, instances_to_select, number_of_features,distance_variable):\n",
    "    features = df.iloc[:, :-1]\n",
    "    labels = df.iloc[:, -1]\n",
    "    rows, columns = features.shape\n",
    "\n",
    "    # Initialize weights to zero\n",
    "    weights = np.zeros(columns)\n",
    "\n",
    "    # Difference between maximum and minimum of each feature for normalization\n",
    "    minimums = np.min(features.values, axis=0)\n",
    "    maximums = np.max(features.values, axis=0)\n",
    "    difference = np.subtract(maximums, minimums)\n",
    "\n",
    "    # Calculate class proportions and determine instances to select per class\n",
    "    class_counts = labels.value_counts()\n",
    "    instances_to_select_per_class = {label: max(1, int(count / class_counts.sum() * instances_to_select))\n",
    "                                     for label, count in class_counts.items()}\n",
    "\n",
    "    for label, count in instances_to_select_per_class.items():\n",
    "        # class_instances = df[df['Label'] == label].index.tolist()\n",
    "        class_instances = df[df.iloc[:, -1] == label].index.tolist()\n",
    "\n",
    "\n",
    "        for _ in range(count):\n",
    "            if len(class_instances) > 0:\n",
    "                random_instance_index = np.random.choice(class_instances)\n",
    "                class_instances.remove(random_instance_index)\n",
    "\n",
    "                random_instance_features = features.iloc[random_instance_index, :].values\n",
    "                random_instance_label = labels[random_instance_index]\n",
    "\n",
    "                # Calculate distances to all other instances using calcDistance\n",
    "                distances = [calcDistance(random_instance_features, features.iloc[i, :].values,distance_variable) for i in range(rows)]\n",
    "\n",
    "                # Identifying nearest hits within the same class\n",
    "                same_class_mask = labels == random_instance_label\n",
    "                same_class_distances = np.array(distances)\n",
    "                same_class_distances[same_class_mask] = np.max(distances) + 1  # Exclude the instance itself\n",
    "                nearest_hits_indices = np.argsort(same_class_distances)[:number_of_neighbours]\n",
    "\n",
    "                # Identifying nearest misses for each different class\n",
    "                nearest_misses_indices = {}\n",
    "                for other_label in class_counts.index:\n",
    "                    if other_label == random_instance_label:\n",
    "                        continue\n",
    "                    other_class_mask = labels == other_label\n",
    "                    other_class_distances = np.array(distances)\n",
    "                    other_class_distances[~other_class_mask] = np.max(distances) + 1\n",
    "                    nearest_misses_indices[other_label] = np.argsort(other_class_distances)[:number_of_neighbours]\n",
    "\n",
    "                # Update weights based on nearest hits\n",
    "                for hit_index in nearest_hits_indices:\n",
    "                    hit_features = features.iloc[hit_index, :].values\n",
    "                    weights -= np.abs(random_instance_features - hit_features) / difference / rows\n",
    "\n",
    "                # Update weights based on nearest misses\n",
    "                for other_label, miss_indices in nearest_misses_indices.items():\n",
    "                    for miss_index in miss_indices:\n",
    "                        miss_features = features.iloc[miss_index, :].values\n",
    "                        weights += np.abs(random_instance_features - miss_features) / difference / rows\n",
    "\n",
    "    # Select top features based on weights\n",
    "    top_features_indices = np.argsort(weights)[-number_of_features:]\n",
    "    top_features = features.columns[top_features_indices]\n",
    "\n",
    "    return top_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is a full implementation of the modified ReliefF algorithm incorporating cost-sensitive learning to address class imbalance. This implementation includes the calculation of class frequencies and their inverses to adjust the feature weight updates accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Frequencies and Weights: Before the loop, the algorithm calculates the frequency of each class and its inverse weight. This makes sure that minority classes are given more importance in the feature weighting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1600,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# def calcDistance(instance1, instance2, distance_variable):\n",
    "#     if distance_variable == 'euclidean':\n",
    "#         return np.sqrt(np.sum((instance1 - instance2) ** 2))\n",
    "#     else:\n",
    "#         # Placeholder for other distance calculations\n",
    "#         return np.linalg.norm(instance1 - instance2)\n",
    "\n",
    "def reliefF_variable_weight(df, number_of_neighbours, instances_to_select, number_of_features, distance_variable):\n",
    "    print(\"inside the reliefF_variable func\")\n",
    "\n",
    "    features = df.iloc[:, :-1]\n",
    "    labels = df.iloc[:, -1]\n",
    "    rows, columns = features.shape\n",
    "\n",
    "    # Initialize weights to zero\n",
    "    weights = np.zeros(columns, dtype=float)\n",
    "\n",
    "    # Unique labels\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    # Calculate class frequencies and inverse weights\n",
    "    class_frequencies = {label: len(np.where(labels == label)[0]) for label in unique_labels}\n",
    "    total_instances = len(labels)\n",
    "    class_weights = {label: total_instances / (len(unique_labels) * freq) for label, freq in class_frequencies.items()}\n",
    "\n",
    "    # Used to select random instance\n",
    "    instances = np.array(list(range(rows)))\n",
    "\n",
    "    # Difference between maximum and minimum of each feature used to calculate diff\n",
    "    minimums = np.min(features.values, axis=0)\n",
    "    maximums = np.max(features.values, axis=0)\n",
    "    difference = np.subtract(maximums, minimums)\n",
    "\n",
    "    total_instances = instances_to_select * len(unique_labels)\n",
    "    label_count = {i: 0 for i in unique_labels}\n",
    "\n",
    "    for i in range(total_instances):\n",
    "        # Choose a random instance and remove from instances to avoid selecting the same thing again\n",
    "        random_instance = np.random.choice(instances)\n",
    "        instances = np.delete(instances, np.where(instances == random_instance))\n",
    "\n",
    "        # Features of random instance used to calculate diff later on\n",
    "        random_instance_features = features.iloc[random_instance, :].values\n",
    "\n",
    "        # Label of random instance and probability of label class\n",
    "        random_instance_label = labels.iloc[random_instance]\n",
    "        probability_random_instance_label = len(np.where(labels == random_instance_label)[0]) / rows\n",
    "\n",
    "        if label_count[random_instance_label] >= instances_to_select:\n",
    "            continue\n",
    "        else:\n",
    "            label_count[random_instance_label] += 1\n",
    "\n",
    "        # Calculate euclidean distance between random instance and all other instances\n",
    "        distances = np.array([calcDistance(random_instance_features, features.iloc[temp, :].values, distance_variable) for temp in instances])\n",
    "\n",
    "        # Sort instances based on distances\n",
    "        sorted_indices = distances.argsort()\n",
    "        sorted_instances = instances[sorted_indices]\n",
    "\n",
    "        # Initialize list of nearest hits for random instance label and dictionary of nearest misses for every other label\n",
    "        nearest_hits = []\n",
    "        nearest_misses = {label: [] for label in unique_labels if label != random_instance_label}\n",
    "\n",
    "        # Finding nearest hits and misses\n",
    "        for temp in sorted_instances:\n",
    "            temp_label = labels.iloc[temp]\n",
    "            if temp_label == random_instance_label and len(nearest_hits) < number_of_neighbours:\n",
    "                nearest_hits.append(temp)\n",
    "            elif temp_label != random_instance_label and len(nearest_misses[temp_label]) < number_of_neighbours:\n",
    "                nearest_misses[temp_label].append(temp)\n",
    "\n",
    "        # Calculate diff for hits\n",
    "        total_hit = np.sum([np.divide(np.abs(np.subtract(random_instance_features, features.iloc[hit, :].values)), difference) for hit in nearest_hits], axis=0)\n",
    "        total_hit *= class_weights[random_instance_label] / (instances_to_select * number_of_neighbours)\n",
    "\n",
    "        # Calculate diff for misses\n",
    "        total_miss = np.zeros(columns, dtype=float)\n",
    "        for label, misses in nearest_misses.items():\n",
    "            temp_miss = np.sum([np.divide(np.abs(np.subtract(random_instance_features, features.iloc[miss, :].values)), difference) for miss in misses], axis=0)\n",
    "            temp_miss *= class_weights[label] / (instances_to_select * number_of_neighbours)\n",
    "            total_miss += temp_miss * (class_frequencies[label] / (rows - class_frequencies[random_instance_label]))\n",
    "\n",
    "        # Update weights\n",
    "        weights += total_miss - total_hit\n",
    "\n",
    "    # Select top features based on weights\n",
    "    ind = np.argpartition(weights, -number_of_features)[-number_of_features:]\n",
    "    top_features = df.columns[ind].tolist()\n",
    "\n",
    "    return top_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1601,
   "metadata": {
    "id": "uVClRo0xfWI2"
   },
   "outputs": [],
   "source": [
    "def reliefF_variable_main(df, number_of_neighbours, instances_to_select, number_of_features):\n",
    "  # print(\"inside the main function\")\n",
    "\n",
    "  features = df.iloc[:,:-1]\n",
    "\n",
    "  braycurtis = []\n",
    "  canberra = []\n",
    "  chebyshev = []\n",
    "  cityblock = []\n",
    "  correlation = []\n",
    "  cosine = []\n",
    "  euclidean = []\n",
    "  jensenshannon = []\n",
    "  sqeuclidean = []\n",
    "\n",
    "  for i in range(len(features)-1):\n",
    "    for j in range(i+1, len(features)):\n",
    "      braycurtis.append(distance.braycurtis(features.iloc[i],features.iloc[j]))\n",
    "      canberra.append(distance.canberra(features.iloc[i],features.iloc[j]))\n",
    "      chebyshev.append(distance.chebyshev(features.iloc[i],features.iloc[j]))\n",
    "      cityblock.append(distance.cityblock(features.iloc[i],features.iloc[j]))\n",
    "      correlation.append(distance.correlation(features.iloc[i],features.iloc[j]))\n",
    "      cosine.append(distance.cosine(features.iloc[i],features.iloc[j]))\n",
    "      euclidean.append(distance.euclidean(features.iloc[i],features.iloc[j]))\n",
    "      jensenshannon.append(distance.jensenshannon(features.iloc[i],features.iloc[j]))\n",
    "      sqeuclidean.append(distance.sqeuclidean(features.iloc[i],features.iloc[j]))\n",
    "  # print(\"braycurtis\")\n",
    "  # print(braycurtis)\n",
    "  # print(\"canberra\")\n",
    "  # print(canberra)\n",
    "  standard_deviation = []\n",
    "  standard_deviation.append(statistics.stdev(braycurtis))\n",
    "  standard_deviation.append(statistics.stdev(canberra))\n",
    "  standard_deviation.append(statistics.stdev(chebyshev))\n",
    "  standard_deviation.append(statistics.stdev(cityblock))\n",
    "  standard_deviation.append(statistics.stdev(correlation))\n",
    "  standard_deviation.append(statistics.stdev(cosine))\n",
    "  standard_deviation.append(statistics.stdev(euclidean))\n",
    "  standard_deviation.append(statistics.stdev(jensenshannon))\n",
    "  standard_deviation.append(statistics.stdev(sqeuclidean))\n",
    "  # print(\"standard dev\") \n",
    "  # print(standard_deviation)\n",
    "\n",
    "  max_value = max(standard_deviation)\n",
    "  # print(\"max_value\")\n",
    "  # print(max_value)\n",
    "  distance_variable = standard_deviation.index(max_value)\n",
    "  \n",
    "  # print(\"distance_variable\")\n",
    "  # print(distance_variable)\n",
    "  #distance_variable = 1\n",
    "  features_combined = []\n",
    "\n",
    "  print(number_of_neighbours)\n",
    "    # print(instances_to_select)\n",
    "    # print(number_of_features) \n",
    "    # print(distance_variable)\n",
    "\n",
    "  for i in range(10):\n",
    "    # print(df)\n",
    "    print(number_of_neighbours)\n",
    "    print(instances_to_select)\n",
    "    print(number_of_features) \n",
    "    print(distance_variable)\n",
    "    a = reliefF_variable(df, number_of_neighbours, instances_to_select, number_of_features, distance_variable)\n",
    "    # a = reliefF_variable_balanced(df, number_of_neighbours, instances_to_select, number_of_features, distance_variable)\n",
    "    # a = reliefF_variable_weight(df, number_of_neighbours, instances_to_select, number_of_features, distance_variable)\n",
    "    # print(a)\n",
    "    features_combined = features_combined + list(a)\n",
    "\n",
    "  features_count = Counter(features_combined)\n",
    "  # print(features_count)\n",
    "\n",
    "  features_count_sorted = sorted(features_count.items(), key=lambda x: x[1], reverse=True)\n",
    "  # print(features_count_sorted)\n",
    "\n",
    "  variable_distance_relieff_features = []\n",
    "  for i in range(number_of_features):\n",
    "    variable_distance_relieff_features.append(features_count_sorted[i][0])\n",
    "  # print(variable_distance_relieff_features)\n",
    "\n",
    "  return variable_distance_relieff_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1602,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reliefF_variable_main_weight(df, number_of_neighbours, instances_to_select, number_of_features):\n",
    "  # print(\"inside the main function\")\n",
    "\n",
    "  features = df.iloc[:,:-1]\n",
    "\n",
    "  braycurtis = []\n",
    "  canberra = []\n",
    "  chebyshev = []\n",
    "  cityblock = []\n",
    "  correlation = []\n",
    "  cosine = []\n",
    "  euclidean = []\n",
    "  jensenshannon = []\n",
    "  sqeuclidean = []\n",
    "\n",
    "  for i in range(len(features)-1):\n",
    "    for j in range(i+1, len(features)):\n",
    "      braycurtis.append(distance.braycurtis(features.iloc[i],features.iloc[j]))\n",
    "      canberra.append(distance.canberra(features.iloc[i],features.iloc[j]))\n",
    "      chebyshev.append(distance.chebyshev(features.iloc[i],features.iloc[j]))\n",
    "      cityblock.append(distance.cityblock(features.iloc[i],features.iloc[j]))\n",
    "      correlation.append(distance.correlation(features.iloc[i],features.iloc[j]))\n",
    "      cosine.append(distance.cosine(features.iloc[i],features.iloc[j]))\n",
    "      euclidean.append(distance.euclidean(features.iloc[i],features.iloc[j]))\n",
    "      jensenshannon.append(distance.jensenshannon(features.iloc[i],features.iloc[j]))\n",
    "      sqeuclidean.append(distance.sqeuclidean(features.iloc[i],features.iloc[j]))\n",
    "  # print(\"braycurtis\")\n",
    "  # print(braycurtis)\n",
    "  # print(\"canberra\")\n",
    "  # print(canberra)\n",
    "  standard_deviation = []\n",
    "  standard_deviation.append(statistics.stdev(braycurtis))\n",
    "  standard_deviation.append(statistics.stdev(canberra))\n",
    "  standard_deviation.append(statistics.stdev(chebyshev))\n",
    "  standard_deviation.append(statistics.stdev(cityblock))\n",
    "  standard_deviation.append(statistics.stdev(correlation))\n",
    "  standard_deviation.append(statistics.stdev(cosine))\n",
    "  standard_deviation.append(statistics.stdev(euclidean))\n",
    "  standard_deviation.append(statistics.stdev(jensenshannon))\n",
    "  standard_deviation.append(statistics.stdev(sqeuclidean))\n",
    "  # print(\"standard dev\") \n",
    "  # print(standard_deviation)\n",
    "\n",
    "  max_value = max(standard_deviation)\n",
    "  # print(\"max_value\")\n",
    "  # print(max_value)\n",
    "  distance_variable = standard_deviation.index(max_value)\n",
    "  \n",
    "  # print(\"distance_variable\")\n",
    "  # print(distance_variable)\n",
    "  #distance_variable = 1\n",
    "  features_combined = []\n",
    "\n",
    "  print(number_of_neighbours)\n",
    "    # print(instances_to_select)\n",
    "    # print(number_of_features) \n",
    "    # print(distance_variable)\n",
    "\n",
    "  for i in range(10):\n",
    "    # print(df)\n",
    "    print(number_of_neighbours)\n",
    "    print(instances_to_select)\n",
    "    print(number_of_features) \n",
    "    print(distance_variable)\n",
    "    # a = reliefF_variable(df, number_of_neighbours, instances_to_select, number_of_features, distance_variable)\n",
    "    # a = reliefF_variable_balanced(df, number_of_neighbours, instances_to_select, number_of_features, distance_variable)\n",
    "    a = reliefF_variable_weight(df, number_of_neighbours, instances_to_select, number_of_features, distance_variable)\n",
    "    # print(a)\n",
    "    features_combined = features_combined + list(a)\n",
    "\n",
    "  features_count = Counter(features_combined)\n",
    "  # print(features_count)\n",
    "\n",
    "  features_count_sorted = sorted(features_count.items(), key=lambda x: x[1], reverse=True)\n",
    "  # print(features_count_sorted)\n",
    "\n",
    "  variable_distance_relieff_features = []\n",
    "  for i in range(number_of_features):\n",
    "    variable_distance_relieff_features.append(features_count_sorted[i][0])\n",
    "  # print(variable_distance_relieff_features)\n",
    "\n",
    "  return variable_distance_relieff_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-validation: Instead of selecting the distance metric based on the highest standard deviation of distances, cross-validation is used to evaluate how well each metric performs in terms of predictive accuracy.  This involves running the reliefF_variable function with each distance metric and evaluating the performance using a classifier. This method ensures the chosen metric is the most effective for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1603,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.ensemble import RandomForestClassifier  # Example classifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "#   # Example classifier\n",
    "\n",
    "# def calcDistance(random_instance_features, temp_features, distance_variable):\n",
    "#   # print(\"inside the calc distance function\")\n",
    "#   # print(\"distance variable is\") \n",
    "#   # print(distance_variable)\n",
    "#   if distance_variable == 0:\n",
    "#     dist = distance.braycurtis(random_instance_features,temp_features)\n",
    "#     return dist\n",
    "#   if distance_variable == 1:\n",
    "#     dist = distance.canberra(random_instance_features,temp_features)\n",
    "#     return dist\n",
    "#   if distance_variable == 2:\n",
    "#     dist = distance.chebyshev(random_instance_features,temp_features)\n",
    "#     return dist\n",
    "#   if distance_variable == 3:\n",
    "#     dist = distance.cityblock(random_instance_features,temp_features)\n",
    "#     return dist\n",
    "#   if distance_variable == 4:\n",
    "#     dist = distance.correlation(random_instance_features,temp_features)\n",
    "#     return dist\n",
    "#   if distance_variable == 5:\n",
    "#     dist = distance.cosine(random_instance_features,temp_features)\n",
    "#     return dist\n",
    "#   if distance_variable == 6:\n",
    "#     dist = distance.euclidean(random_instance_features,temp_features)\n",
    "#     return dist\n",
    "#   if distance_variable == 7:\n",
    "#     dist = distance.jensenshannon(random_instance_features,temp_features)\n",
    "#     return dist\n",
    "#   if distance_variable == 8:\n",
    "#     dist = distance.sqeuclidean(random_instance_features,temp_features)\n",
    "#     return dist\n",
    "#   return 0\n",
    "\n",
    "# def reliefF_variable(df, number_of_neighbours, instances_to_select, number_of_features, distance_variable):\n",
    "#   print(\"SSSSSSSSSSSSSSSSSSS\")\n",
    "#   print(\"inside the reliefF_variable func\")\n",
    "\n",
    "#   features = df.iloc[:,:-1]\n",
    "#   labels = df.iloc[:,-1]\n",
    "#   rows,columns = features.shape\n",
    "\n",
    "#   #initialize weights to zero\n",
    "#   weights = np.zeros(columns,dtype = 'int')\n",
    "\n",
    "#   #unique labels\n",
    "#   unique_labels = np.unique(labels)\n",
    "\n",
    "#   #used to select random instance\n",
    "#   instances=np.array(list(range(1,rows)))\n",
    "\n",
    "#   #difference between maximum and minimum of each feature used to calculate diff\n",
    "#   minimums=np.min(features.values,axis=0)\n",
    "#   maximums=np.max(features.values,axis=0)\n",
    "#   difference=np.subtract(maximums,minimums)\n",
    "  \n",
    "#   total_instances = instances_to_select * len(unique_labels)\n",
    "#   label_count = {}\n",
    "\n",
    "#   for i in unique_labels:\n",
    "#     label_count[i] = 0\n",
    "\n",
    "#   for i in range(total_instances):\n",
    "\n",
    "#     #choose a random instance and remove from instances to avoid selecting same thing again\n",
    "#     random_instance = np.random.choice(instances[:-1])\n",
    "#     instances=np.delete(instances,np.where(instances==random_instance))\n",
    "\n",
    "#     #features of random instance used to calculate diff later on\n",
    "#     random_instance_features = features.iloc[random_instance,:].values\n",
    "\n",
    "#     #label of random instance and probability of label class\n",
    "#     random_instance_label = labels[random_instance]\n",
    "#     probability_random_instance_label = len(np.where(labels==random_instance_label)[0])/rows\n",
    "\n",
    "#     if label_count[random_instance_label] >= instances_to_select:\n",
    "#       i = i-1\n",
    "#       continue\n",
    "    \n",
    "#     else:\n",
    "#       label_count[random_instance_label] = label_count[random_instance_label] + 1\n",
    "\n",
    "#     #calculate euclidean distance between random instance and all other instances\n",
    "#     distances = []\n",
    "#     for temp in instances:\n",
    "#       temp_features = features.iloc[temp,:].values\n",
    "#       dist = calcDistance(random_instance_features, temp_features, distance_variable)\n",
    "#       distances.append(dist)\n",
    "    \n",
    "#     #sort instances based on distances\n",
    "#     distances = np.array(distances)\n",
    "#     arr1inds = distances.argsort()\n",
    "#     sorted_distances = distances[arr1inds[::]]\n",
    "#     sorted_instances = instances[arr1inds[::]]\n",
    "\n",
    "#     #initialize list of nearest hits for random instance label and dictionary of nearest misses for every other label\n",
    "#     nearest_hits = []\n",
    "#     nearest_misses = {}\n",
    "\n",
    "#     #finding nearest hits for random instance label\n",
    "#     for temp in sorted_instances:\n",
    "#       if labels[temp] == random_instance_label:\n",
    "#         nearest_hits.append(temp)\n",
    "#       if len(nearest_hits) == number_of_neighbours:\n",
    "#         break\n",
    "      \n",
    "#     #finding nearest misses for all other labels\n",
    "#     for x in unique_labels:\n",
    "#       if x == random_instance_label:\n",
    "#         continue      \n",
    "#       nearest_misses[x] = []\n",
    "#       for temp in sorted_instances:\n",
    "#         if labels[temp] == x:\n",
    "#           nearest_misses[x].append(temp)\n",
    "#         if len(nearest_misses[x]) == number_of_neighbours:\n",
    "#           break\n",
    "\n",
    "#     #used to find sum of diff function in weights equation for hits\n",
    "#     total_hit = np.zeros(columns,dtype='int')\n",
    "\n",
    "#     #find sum of diff function in weights equation for hits\n",
    "#     for hit in range(len(nearest_hits)):\n",
    "#       hI = features.iloc[nearest_hits[hit],:].values\n",
    "#       dRH = np.divide(np.abs(np.subtract(random_instance_features,hI)),difference)\n",
    "#       dRH = dRH/(instances_to_select * number_of_neighbours)\n",
    "#       total_hit = np.add(total_hit,dRH)\n",
    "\n",
    "#     #used to find sum of diff function in weights equation for misses\n",
    "#     total_miss=np.zeros(columns,dtype='int')\n",
    "\n",
    "#     #find sum of diff function in weights equation for misses in each class\n",
    "#     for each_label in nearest_misses:\n",
    "#       temp_miss=np.zeros(columns,dtype='int')\n",
    "#       pclass=len(np.where(labels==each_label)[0])/rows #getting the probability of getting this class\n",
    "#       postProb=pclass/(1-probability_random_instance_label) #calculating the posterior probability of getting this class\n",
    "\n",
    "#       for each_miss in nearest_misses[each_label]:\n",
    "#         mI = features.iloc[each_miss,:].values\n",
    "#         dRM = np.divide(np.abs(np.subtract(random_instance_features,mI)),difference)\n",
    "#         dRM = dRM/(instances_to_select * number_of_neighbours)\n",
    "#         temp_miss = np.add(temp_miss,dRM)\n",
    "\n",
    "#       total_miss = np.add(total_miss,(temp_miss*postProb))\n",
    "    \n",
    "#     #update value of weights based on total hits and total miss and diff function values\n",
    "#     weights=np.add(weights,total_miss)\n",
    "#     weights=np.subtract(weights,total_hit) \n",
    "    \n",
    "\n",
    "#   #select number_of_features weights with highest values and sort\n",
    "#   ind = np.argpartition(weights, -number_of_features)[-number_of_features:]\n",
    "#   ind = np.sort(ind)[::-1]\n",
    "\n",
    "#   #column names of data frame\n",
    "#   feature_names = list(df.columns.values)\n",
    "#   feature_names = np.array(feature_names)\n",
    "\n",
    "#   #top features based on weights\n",
    "#   top_features = feature_names[ind]\n",
    "\n",
    "#   return top_features\n",
    "# # \n",
    "\n",
    "# def reliefF_variable_crossvalidated(df, number_of_neighbours, instances_to_select, number_of_features, distance_variable, cv=5):\n",
    "#     \"\"\"\n",
    "#     Evaluate ReliefF variable selection with cross-validation for a given distance metric.\n",
    "    \n",
    "#     \"\"\"\n",
    "#     print(\"inside the releifF_variable_crossvalidated\")\n",
    "#     print(\"distance variable value is\")\n",
    "#     print(distance_variable)\n",
    "    \n",
    "#     selected_features = reliefF_variable(df, number_of_neighbours, instances_to_select, number_of_features, distance_variable)\n",
    "#     # print(\"selected_features\")\n",
    "#     # print(selected_features)\n",
    "    \n",
    "#     X = df[selected_features].values\n",
    "#     y = df.iloc[:,-1].values\n",
    "#     classifier = GradientBoostingClassifier()  # Example classifier, can be replaced\n",
    "#     scores = cross_val_score(classifier, X, y, cv=cv)\n",
    "#     print(\"scores value\")\n",
    "#     print(scores)\n",
    "#     return np.mean(scores), selected_features\n",
    "\n",
    "# def adaptive_distance_metric_selection(df, number_of_neighbours, instances_to_select, number_of_features):\n",
    "#     print(\"inside adaptive distance_metric_selection\")\n",
    "#     cv = 5\n",
    "#     metrics_performance = {}\n",
    "#     for distance_variable in range(9):  # For each distance metric\n",
    "#         print(\"distance_variable\")\n",
    "#         print(distance_variable)\n",
    "#         score, _ = reliefF_variable_crossvalidated(df, number_of_neighbours, instances_to_select, number_of_features, distance_variable, cv)\n",
    "#         print(\"score\")\n",
    "#         print(score)\n",
    "#         metrics_performance[distance_variable] = score\n",
    "#     print(\"metrics performance\")\n",
    "#     print(metrics_performance)\n",
    "    \n",
    "#     # Select the best performing metric\n",
    "#     best_metric = max(metrics_performance, key=metrics_performance.get)\n",
    "#     print(\"best_metric\")\n",
    "#     print(best_metric)\n",
    "#     features_combined = []\n",
    "#     best_score, best_features = reliefF_variable_crossvalidated(df, number_of_neighbours, instances_to_select, number_of_features, best_metric, cv)\n",
    "#   #   for i in range(10):\n",
    "        \n",
    "#   #       a = reliefF_variable(df, number_of_neighbours, instances_to_select, number_of_features, distance_variable)\n",
    "#   #   # print(a)\n",
    "#   #       features_combined = features_combined + list(a)\n",
    "\n",
    "#   #   features_count = Counter(features_combined)\n",
    "\n",
    "#   #   features_count_sorted = sorted(features_count.items(), key=lambda x: x[1], reverse=True)\n",
    "#   # # print(features_count_sorted)\n",
    "\n",
    "#   #   variable_distance_relieff_two_features = []\n",
    "#   #   for i in range(number_of_features):\n",
    "#   #       variable_distance_relieff_two_features.append(features_count_sorted[i][0])\n",
    "#   # # print(variable_distance_relieff_features)\n",
    "\n",
    "#   #   return variable_distance_relieff_two_features    \n",
    "    \n",
    "\n",
    "#     # print(best_features)\n",
    "    \n",
    "#     # print(f\"Best distance metric: {best_metric} with cross-validated score: {best_score}\")\n",
    "#     return best_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The calcDistance function is updated. This function is modified to calculate the distance between two instances using a different metric for each feature, based on a list of metrics provided. \n",
    "##### The reliefF_variable function is adjusted. This function is modified to use the updated calcDistance function. The distance_variable parameter now receives a list of metrics corresponding to each feature rather than a single metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1604,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial import distance\n",
    "# import numpy as np\n",
    "\n",
    "# def calcDistance_feature_specific(value1, value2, metric_index):\n",
    "#     # Convert scalar values to arrays for compatibility with distance functions\n",
    "#     value1, value2 = np.array([value1]), np.array([value2])\n",
    "    \n",
    "#     # Select and apply the appropriate distance metric\n",
    "#     if metric_index == 0:\n",
    "#         # Bray-Curtis distance\n",
    "#         dist = distance.braycurtis(value1, value2)\n",
    "#     elif metric_index == 1:\n",
    "#         # Canberra distance\n",
    "#         dist = distance.canberra(value1, value2)\n",
    "#     elif metric_index == 2:\n",
    "#         # Chebyshev distance\n",
    "#         dist = distance.chebyshev(value1, value2)\n",
    "#     elif metric_index == 3:\n",
    "#         # City Block (Manhattan) distance\n",
    "#         dist = distance.cityblock(value1, value2)\n",
    "#     elif metric_index == 4:\n",
    "#         # Correlation distance\n",
    "#         dist = distance.correlation(value1, value2)\n",
    "#     elif metric_index == 5:\n",
    "#         # Cosine distance\n",
    "#         dist = distance.cosine(value1, value2)\n",
    "#     elif metric_index == 6:\n",
    "#         # Euclidean distance\n",
    "#         dist = distance.euclidean(value1, value2)\n",
    "#     elif metric_index == 7:\n",
    "#         # Jensen-Shannon distance, note: requires probability distributions\n",
    "#         # Here, you need to ensure the inputs are suitable for Jensen-Shannon,\n",
    "#         # for simplicity, this example treats inputs as distributions\n",
    "#         dist = distance.jensenshannon(value1, value2, base=2)\n",
    "#     elif metric_index == 8:\n",
    "#         # Squared Euclidean distance\n",
    "#         # Not directly available in scipy.spatial.distance, so compute manually\n",
    "#         dist = np.sum((value1 - value2) ** 2)\n",
    "#     else:\n",
    "#         # Default case or error handling\n",
    "#         raise ValueError(\"Unsupported metric_index: {}\".format(metric_index))\n",
    "    \n",
    "#     # Handle cases where the distance calculation returns nan or inf\n",
    "#     if np.isnan(dist) or np.isinf(dist):\n",
    "#         return 0.0\n",
    "#     return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1605,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def reliefF_variable_custom(df, number_of_neighbours, instances_to_select, number_of_features, distance_variables_per_feature):\n",
    "#     features = df.iloc[:, :-1]\n",
    "#     labels = df.iloc[:, -1]\n",
    "#     rows, columns = features.shape\n",
    "\n",
    "#     # Initialize weights to zero\n",
    "#     weights = np.zeros(columns, dtype='float')\n",
    "\n",
    "#     # Unique labels in the dataset\n",
    "#     unique_labels = np.unique(labels)\n",
    "\n",
    "#     # For storing minimum and maximum of each feature for normalization\n",
    "#     minimums = np.min(features.values, axis=0)\n",
    "#     maximums = np.max(features.values, axis=0)\n",
    "#     difference = maximums - minimums\n",
    "\n",
    "#     for _ in range(instances_to_select):\n",
    "#         # Select a random instance\n",
    "#         random_index = np.random.randint(rows)\n",
    "#         random_instance_features = features.iloc[random_index, :].values\n",
    "#         random_instance_label = labels[random_index]\n",
    "\n",
    "#         # Calculate distances to all other instances\n",
    "#         distances = np.zeros((rows, columns))\n",
    "#         for i in range(rows):\n",
    "#             for j in range(columns):\n",
    "#                 distance_metric_index = distance_variables_per_feature[j]\n",
    "#                 distances[i, j] = calcDistance_feature_specific(random_instance_features[j], features.iloc[i, j], distance_metric_index)\n",
    "\n",
    "#         # Find nearest hits and misses\n",
    "#         hits_mask = labels == random_instance_label\n",
    "#         misses_mask = ~hits_mask\n",
    "#         hits_distances = np.where(hits_mask, distances, np.inf).min(axis=0)\n",
    "#         misses_distances = np.where(misses_mask, distances, np.inf).min(axis=0)\n",
    "\n",
    "#         # Update weights\n",
    "#         weights += misses_distances - hits_distances\n",
    "\n",
    "#     # Normalize weights\n",
    "#     weights = (weights - np.min(weights)) / (np.max(weights) - np.min(weights))\n",
    "\n",
    "#     # Select top features based on weights\n",
    "#     top_feature_indices = np.argsort(weights)[-number_of_features:]\n",
    "\n",
    "#     # Map indices to feature names\n",
    "#     top_features = features.columns[top_feature_indices].tolist()\n",
    "\n",
    "#     return top_features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1606,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from collections import Counter\n",
    "\n",
    "def calcDistanceCustom(feature_a, feature_b, metric):\n",
    "    if metric == 0:\n",
    "        return distance.braycurtis(feature_a, feature_b)\n",
    "    elif metric == 1:\n",
    "        return distance.canberra(feature_a, feature_b)\n",
    "    elif metric == 2:\n",
    "        return distance.chebyshev(feature_a, feature_b)\n",
    "    elif metric == 3:\n",
    "        return distance.cityblock(feature_a, feature_b)\n",
    "    elif metric == 4:\n",
    "        return distance.correlation(feature_a, feature_b)\n",
    "    elif metric == 5:\n",
    "        return distance.cosine(feature_a, feature_b)\n",
    "    elif metric == 6:\n",
    "        return distance.euclidean(feature_a, feature_b)\n",
    "    elif metric == 7:\n",
    "        return distance.jensenshannon(feature_a, feature_b)\n",
    "    elif metric == 8:\n",
    "        return distance.sqeuclidean(feature_a, feature_b)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported distance metric\")\n",
    "\n",
    "def reliefF_variable_custom(df, number_of_neighbours, instances_to_select, number_of_features, distance_variables_per_feature):\n",
    "    features = df.iloc[:, :-1]\n",
    "    labels = df.iloc[:, -1]\n",
    "    rows, columns = features.shape\n",
    "    weights = np.zeros(columns)\n",
    "\n",
    "    for i in range(instances_to_select):\n",
    "        random_instance_index = np.random.randint(0, rows)\n",
    "        random_instance_features = features.iloc[random_instance_index, :].values\n",
    "        random_instance_label = labels[random_instance_index]\n",
    "\n",
    "        hit_distances = np.zeros(columns)\n",
    "        miss_distances = {label: np.zeros(columns) for label in np.unique(labels)}\n",
    "\n",
    "        for other_index, (other_features, other_label) in enumerate(zip(features.values, labels)):\n",
    "            if other_index == random_instance_index:\n",
    "                continue\n",
    "\n",
    "            # Calculate distances feature-wise using the specified metric for each feature\n",
    "            for feature_index in range(columns):\n",
    "                metric = distance_variables_per_feature[feature_index]\n",
    "                dist = calcDistanceCustom([random_instance_features[feature_index]], [other_features[feature_index]], metric)\n",
    "\n",
    "                if other_label == random_instance_label:\n",
    "                    hit_distances[feature_index] += dist\n",
    "                else:\n",
    "                    miss_distances[other_label][feature_index] += dist\n",
    "\n",
    "        # Update weights\n",
    "        for label, miss_dist in miss_distances.items():\n",
    "            # Normalize distances by the number of instances\n",
    "            norm_hit_dist = hit_distances / number_of_neighbours\n",
    "            norm_miss_dist = miss_dist / number_of_neighbours\n",
    "\n",
    "            # Update weights: Increase weight for features where misses are far and hits are close\n",
    "            weights += norm_miss_dist - norm_hit_dist\n",
    "\n",
    "    # Select top features based on weights\n",
    "    top_feature_indices = np.argsort(weights)[-number_of_features:]\n",
    "    feature_names = df.columns[:-1][top_feature_indices]\n",
    "\n",
    "    return feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1607,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import statistics\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_distances(feature_values):\n",
    "    # Initialize lists to store distances\n",
    "    print(\"inside calculate_distances\")\n",
    "    braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, jensenshannon, sqeuclidean = ([] for i in range(9))\n",
    "    \n",
    "    # Calculate distances for each pair of instances for a single feature\n",
    "    for i in range(len(feature_values)-1):\n",
    "        for j in range(i+1, len(feature_values)):\n",
    "            # Adapt these calculations for single feature comparisons\n",
    "            braycurtis.append(distance.braycurtis([feature_values[i]], [feature_values[j]]))\n",
    "            canberra.append(distance.canberra([feature_values[i]], [feature_values[j]]))\n",
    "            chebyshev.append(distance.chebyshev([feature_values[i]], [feature_values[j]]))\n",
    "            cityblock.append(distance.cityblock([feature_values[i]], [feature_values[j]]))\n",
    "            correlation.append(distance.correlation([feature_values[i]], [feature_values[j]]))\n",
    "            cosine.append(distance.cosine([feature_values[i]], [feature_values[j]]))\n",
    "            euclidean.append(distance.euclidean([feature_values[i]], [feature_values[j]]))\n",
    "            jensenshannon.append(distance.jensenshannon([feature_values[i]], [feature_values[j]], base=2))\n",
    "            sqeuclidean.append(distance.sqeuclidean([feature_values[i]], [feature_values[j]]))\n",
    "    \n",
    "    return [braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, jensenshannon, sqeuclidean]\n",
    "    # return [braycurtis, canberra]\n",
    "\n",
    "\n",
    "def find_most_variable_distance(distances):\n",
    "    # Calculate standard deviation for each distance metric\n",
    "    # standard_deviations = [statistics.stdev(list(d) if d else 0 for d in distances]\n",
    "    # standard_deviations = [statistics.stdev(list(d)) if d else 0 for d in distances]\n",
    "    # standard_deviations = [statistics.stdev(list(filter(np.isfinite, np.nditer(d)))) if d else 0 for d in distances]\n",
    "    # standard_deviations = [statistics.stdev(list(filter(np.isfinite, d))) if d else 0 for d in distances]\n",
    "    standard_deviations = [statistics.stdev(list(filter(np.isfinite, d))) if len(list(filter(np.isfinite, d))) >= 2 else 0 for d in distances]\n",
    "\n",
    "\n",
    "\n",
    "    # Find the index of the maximum standard deviation\n",
    "    max_std_dev_index = standard_deviations.index(max(standard_deviations))\n",
    "    return max_std_dev_index\n",
    "\n",
    "# Helper function to calculate most variable distances\n",
    "def calculate_most_variable_distances(features):\n",
    "    print(\"inside calculate_most_variable_distances\")\n",
    "    count = 0 \n",
    "\n",
    "    distance_variables_per_feature = {}\n",
    "    for feature_index in range(features.shape[1]):\n",
    "        current_feature_values = features.iloc[:, feature_index].values\n",
    "        distances = calculate_distances(current_feature_values)\n",
    "        print(\"distances\")\n",
    "        \n",
    "        most_variable_distance_index = find_most_variable_distance(distances)\n",
    "        distance_variables_per_feature[feature_index] = most_variable_distance_index\n",
    "        print(count)\n",
    "        count = count + 1 \n",
    "    return distance_variables_per_feature\n",
    "\n",
    "\n",
    "def reliefF_variable_main_modified(df, number_of_neighbours, instances_to_select, number_of_features):\n",
    "    print(\"inside the main function\")\n",
    "    \n",
    "    features = df.iloc[:,:-1]\n",
    "    num_features = features.shape[1]\n",
    "    \n",
    "    # Calculate the most variable distance metric for each feature\n",
    "    distance_variables_per_feature = calculate_most_variable_distances(features)\n",
    "    \n",
    "    # This assumes reliefF_variable is adapted as outlined\n",
    "    features_combined = []\n",
    "    for i in range(10):  # Example iteration count; adjust as needed\n",
    "        selected_features = reliefF_variable_custom(df, number_of_neighbours, instances_to_select, number_of_features, distance_variables_per_feature)\n",
    "        features_combined += list(selected_features)\n",
    "    \n",
    "    # Aggregate and select top features based on frequency of selection\n",
    "    features_count = Counter(features_combined)\n",
    "    features_count_sorted = sorted(features_count.items(), key=lambda x: x[1], reverse=True)\n",
    "    variable_distance_relieff_features = [feature for feature, count in features_count_sorted[:number_of_features]]\n",
    "    \n",
    "    return variable_distance_relieff_features\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPfOhCjk2YG4"
   },
   "source": [
    "# Comparing Feature Selection Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jn2FuSc4ynAi"
   },
   "source": [
    "## Control Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1608,
   "metadata": {
    "cellView": "code",
    "id": "INFtxUdEDblS"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "def normalize_data(dataframe):\n",
    "  dataframe=(dataframe-dataframe.mean())/dataframe.std()\n",
    "  dataframe=(dataframe-dataframe.min())/(dataframe.max()-dataframe.min())\n",
    "  dataframe[np.isnan(dataframe)] = 0\n",
    "  return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1609,
   "metadata": {
    "cellView": "code",
    "id": "eyzkbPuL44As"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "def plot_roc_auc(x_test, y_test, model):\n",
    "  #define metrics\n",
    "  y_pred_proba = model.predict_proba(x_test)[::,1]\n",
    "  y_pred_proba = normalize_data(y_pred_proba)\n",
    "  fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "  auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "  rocauc_score = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "  #print('The ROCAUC is {}'.format(rocauc_score))\n",
    "\n",
    "\n",
    "  #create ROC curve\n",
    "  #plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n",
    "  #plt.ylabel('True Positive Rate')\n",
    "  #plt.xlabel('False Positive Rate')\n",
    "  #plt.legend(loc=4)\n",
    "  #plt.show()\n",
    "\n",
    "  return rocauc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1610,
   "metadata": {
    "cellView": "code",
    "id": "YIxUaDRqx-UZ"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "def training_model(train, test, fold_no, model):\n",
    "  x_train = train.drop(['label'],axis=1)\n",
    "  y_train = train.label\n",
    "  x_test = test.drop(['label'],axis=1)\n",
    "  y_test = test.label\n",
    "  model.fit(x_train, y_train)\n",
    "  \n",
    "  x_test = np.ascontiguousarray(x_test)\n",
    "  y_test = np.ascontiguousarray(y_test)\n",
    "\n",
    "  score = model.score(x_test,y_test)\n",
    "  scores_list = []\n",
    "\n",
    "  #print('For Fold {} the accuracy is {}'.format(str(fold_no),score))\n",
    "  scores_list.append(score)\n",
    "\n",
    "  f1_scores = f1_score(y_test,model.predict(x_test))\n",
    "  #print('F1 score is {}'.format(f1_scores))\n",
    "  scores_list.append(f1_scores)\n",
    "\n",
    "  unique_labels = np.unique(y_test)\n",
    "  predicted = model.predict(x_test)\n",
    "  #y_test = list(y_test)\n",
    "  #print(y_test)\n",
    "\n",
    "  '''\n",
    "\n",
    "  recall = []\n",
    "  for k in unique_labels:\n",
    "    indices = [int(i) for i, x in enumerate(y_test) if x == k]\n",
    "    res_list_pred = list(map(predicted.__getitem__, indices))\n",
    "    res_list_test = list(map(list(y_test).__getitem__, indices))\n",
    "    recall_class = recall_score(res_list_pred, res_list_test)\n",
    "    recall.append(recall_class)\n",
    "  \n",
    "  recall = [0.000001 if x == 0 else x for x in recall]\n",
    "  geometric_mean = gmean(recall)\n",
    "\n",
    "  '''\n",
    "\n",
    "\n",
    "  geometric_mean = geometric_mean_score(y_test,model.predict(x_test))\n",
    "  #print('The geometric mean is {}'.format(geometric_mean))\n",
    "  scores_list.append(geometric_mean)\n",
    "\n",
    "  roc_scores = plot_roc_auc(x_test, y_test,model)\n",
    "  scores_list.append(roc_scores)\n",
    "\n",
    "  # Calculate precision\n",
    "  precision_scores = precision_score(y_test, predicted)\n",
    "  scores_list.append(precision_scores)\n",
    "  \n",
    "    \n",
    "  # Calculate recall\n",
    "  recall_scores = recall_score(y_test, predicted)\n",
    "  scores_list.append(recall_scores)\n",
    "\n",
    "  return scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1611,
   "metadata": {
    "id": "O8Uqn8__ywX3"
   },
   "outputs": [],
   "source": [
    "def control_function(dataframe, n_of_splits, df_name, path_name, df_scores):\n",
    "  #dataframe = dataframe.reset_index()\n",
    "  #dataframe = dataframe.replace(np.inf, dataframe.mean())\n",
    "  #dataframe = dataframe.fillna(dataframe.mean())\n",
    "  skf = StratifiedKFold(n_splits=n_of_splits)\n",
    "  x = dataframe\n",
    "  y = dataframe.label\n",
    "\n",
    "  svc_model = SVC(kernel='rbf',probability=True)\n",
    "# rbf \n",
    "  dtc_model = DecisionTreeClassifier(random_state = 0)\n",
    "  rfc_model = RandomForestClassifier(random_state=0)\n",
    "  naive_bayes_model = GaussianNB()\n",
    "  gradient_boosting_model = GradientBoostingClassifier(random_state=0)\n",
    "  knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "  # dtc_model = DecisionTreeClassifier(random_state = 0)\n",
    "\n",
    "  models = [svc_model, dtc_model, rfc_model, naive_bayes_model, gradient_boosting_model, knn_model]\n",
    "  models_name = ['Support Vector Classifier', 'Decision Tree Classifier', 'Random Forest Classifier', 'Gaussian Naive Bayes', 'Gradient Boosting Classifier', 'K Nearest Neighbour']\n",
    "  # models = [dtc_model]\n",
    "  # models_name = ['Decision Tree Classifier']\n",
    "\n",
    "\n",
    "  for i in range(len(models)):\n",
    "    print(models_name[i], \" + \", df_name, \" + \", path_name, \"\\n\")    \n",
    "    fold_no = 1\n",
    "    for train_index,test_index in skf.split(x, y):\n",
    "      train = dataframe.iloc[train_index,:]\n",
    "      test = dataframe.iloc[test_index,:]\n",
    "      model_scores = training_model(train, test, fold_no, models[i])\n",
    "      df_scores.loc[0 if pd.isnull(df_scores.index.max()) else df_scores.index.max() + 1] = [df_name, path_name, models_name[i], fold_no] + model_scores\n",
    "      fold_no += 1\n",
    "      #print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1612,
   "metadata": {
    "id": "JqLR4zTMAvAp"
   },
   "outputs": [],
   "source": [
    "def normalize_dataframe(dataframe):\n",
    "  dataframe=(dataframe-dataframe.mean())/dataframe.std()\n",
    "  dataframe=(dataframe-dataframe.min())/(dataframe.max()-dataframe.min())\n",
    "  dataframe.fillna(0,inplace=True)\n",
    "  return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1613,
   "metadata": {
    "id": "w0ZOMXBS1Uc5"
   },
   "outputs": [],
   "source": [
    "def main_function(path_name, df_scores, feature_names_path, count):\n",
    "  df = pd.read_csv(path_name)\n",
    "  print(\"df shapes\")\n",
    "  print(df.shape)\n",
    "  num_rows, num_columns = df.shape\n",
    "  \n",
    "  # num_features = int(num_columns * 0.70)\n",
    "\n",
    "  # number_of_neighbours = 5\n",
    "  # instances_to_select = 10\n",
    "  # Combination A ( Using more features) \n",
    "  num_features = int(num_columns * 0.55)\n",
    "\n",
    "  number_of_neighbours = 7\n",
    "  instances_to_select = 10\n",
    "\n",
    "\n",
    "\n",
    "  # # Combination B ( Using more instances) \n",
    "  # num_features = int(num_columns * 0.70)\n",
    "\n",
    "  # number_of_neighbours = 5\n",
    "  # instances_to_select = 15\n",
    "\n",
    "\n",
    "  # # Combination C ( Using more numder of neighbors) \n",
    "  # num_features = int(num_columns * 0.70)\n",
    "\n",
    "  # number_of_neighbours = 8\n",
    "  # instances_to_select = 10\n",
    "\n",
    "    \n",
    "  number_of_features = num_features\n",
    "\n",
    "  \n",
    "  df = normalize_dataframe(df)\n",
    "\n",
    "  #Chi-Square\n",
    "  chisquare_features = chi_square(df, number_of_features)\n",
    "  df_chisquare = pd.read_csv(path_name, usecols = chisquare_features)\n",
    "  df_chisquare = normalize_dataframe(df_chisquare)\n",
    "  print('chi square') \n",
    " \n",
    "  \n",
    "\n",
    "  #ReliefF\n",
    "  reliefF_features = reliefF(df, number_of_neighbours, instances_to_select, number_of_features)\n",
    "  df_reliefF = pd.read_csv(path_name, usecols = reliefF_features)\n",
    "  df_reliefF = normalize_dataframe(df_reliefF)\n",
    "  print('reliefF')\n",
    "  \n",
    "  \n",
    "\n",
    "  #SVM-RFE\n",
    "  svmrfe_features = svmrfe(df, number_of_features)\n",
    "  df_svmrfe = pd.read_csv(path_name, usecols = svmrfe_features)\n",
    "  df_svmrfe = normalize_dataframe(df_svmrfe)\n",
    "  print('SVM')\n",
    " \n",
    "  \n",
    "\n",
    "  #Variable-ReliefF\n",
    "  print(number_of_neighbours)\n",
    "  variable_distance_relieff_features = reliefF_variable_main(df, number_of_neighbours, instances_to_select, number_of_features)\n",
    "  df_variable_reliefF = pd.read_csv(path_name, usecols = variable_distance_relieff_features)\n",
    "  df_variable_reliefF = normalize_dataframe(df_variable_reliefF)\n",
    "  print('variable rfe') \n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "  #Original\n",
    "  df_original = pd.read_csv(path_name).iloc[:,:-1]\n",
    "  df_original = normalize_dataframe(df_original)\n",
    "  print('df_original')\n",
    "# adaptive_distance_metric_selection\n",
    "  # Variable-ReliefF\n",
    "  # variable_distance_relieffTwo_features = adaptive_distance_metric_selection(df, number_of_neighbours, instances_to_select, number_of_features)\n",
    "  # df_variable_reliefFTwo = pd.read_csv(path_name, usecols = variable_distance_relieffTwo_features)\n",
    "  # df_variable_reliefFTwo = normalize_dataframe(df_variable_reliefFTwo)\n",
    "  # print('variable reliefFTwo') \n",
    "\n",
    "  variable_distance_relieffTwo_features = reliefF_variable_main_weight(df, number_of_neighbours, instances_to_select, number_of_features)\n",
    "  df_variable_reliefFTwo = pd.read_csv(path_name, usecols = variable_distance_relieffTwo_features)\n",
    "  df_variable_reliefFTwo = normalize_dataframe(df_variable_reliefFTwo)\n",
    "  print('variable reliefFTwo') \n",
    "\n",
    "  features = pd.DataFrame(\n",
    "    {'svmrfe': svmrfe_features,\n",
    "     'chisquare': chisquare_features,\n",
    "     'relief': reliefF_features,\n",
    "     'variable': variable_distance_relieff_features,\n",
    "     'reliefFTwo': variable_distance_relieffTwo_features,\n",
    "\n",
    "    })\n",
    "    \n",
    "  # feature_names_path = feature_names_path + str(count)\n",
    "  # features.to_csv(feature_names_path)\n",
    "\n",
    "  features = pd.DataFrame(\n",
    "    {\n",
    "     'reliefFTwo': variable_distance_relieffTwo_features,\n",
    "    })\n",
    "  # features = pd.DataFrame(\n",
    "  #   {\n",
    "  #    'variable': variable_distance_relieff_features,\n",
    "  #   })\n",
    "  feature_names_path = feature_names_path + str(count)\n",
    "  features.to_csv(feature_names_path)\n",
    "\n",
    "\n",
    "  #Labels\n",
    "  df_labels = pd.read_csv(path_name).iloc[:,-1]\n",
    "  df_original['label'] = df_labels\n",
    "  df_reliefF['label'] = df_labels\n",
    "  df_chisquare['label'] = df_labels\n",
    "  df_svmrfe['label'] = df_labels\n",
    "  df_variable_reliefF['label'] = df_labels\n",
    "  df_variable_reliefFTwo['label'] = df_labels\n",
    "\n",
    "\n",
    "  n_of_splits = 5\n",
    "  cr = 1\n",
    "\n",
    "  df_list = [df_original, df_reliefF, df_chisquare, df_svmrfe, df_variable_reliefF,df_variable_reliefFTwo]\n",
    "  df_list_name = ['Original', 'ReliefF', 'ChiSquare', 'SVMRFE', 'Variable ReliefF', 'ReliefFTwo']\n",
    "  # df_list = [df_variable_reliefFTwo]\n",
    "  # df_list_name = [ 'ReliefFTwo']\n",
    "\n",
    "  for df_index in range(len(df_list)):\n",
    "    print(\"cr\")\n",
    "    print(cr)\n",
    "    control_function(df_list[df_index], n_of_splits, df_list_name[df_index], path_name, df_scores)\n",
    "    cr = cr + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1614,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d7SC1NisLNm",
    "outputId": "dc1d7b08-059d-4517-c535-549800a79317",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shapes\n",
      "(64, 85)\n",
      "chi square\n",
      "reliefF\n",
      "SVM\n",
      "7\n",
      "7\n",
      "7\n",
      "10\n",
      "46\n",
      "1\n",
      "SSSSSSSSSSSSSSSSSSS\n",
      "inside the reliefF_variable func2\n",
      "7\n",
      "10\n",
      "46\n",
      "1\n",
      "SSSSSSSSSSSSSSSSSSS\n",
      "inside the reliefF_variable func2\n",
      "7\n",
      "10\n",
      "46\n",
      "1\n",
      "SSSSSSSSSSSSSSSSSSS\n",
      "inside the reliefF_variable func2\n",
      "7\n",
      "10\n",
      "46\n",
      "1\n",
      "SSSSSSSSSSSSSSSSSSS\n",
      "inside the reliefF_variable func2\n",
      "7\n",
      "10\n",
      "46\n",
      "1\n",
      "SSSSSSSSSSSSSSSSSSS\n",
      "inside the reliefF_variable func2\n",
      "7\n",
      "10\n",
      "46\n",
      "1\n",
      "SSSSSSSSSSSSSSSSSSS\n",
      "inside the reliefF_variable func2\n",
      "7\n",
      "10\n",
      "46\n",
      "1\n",
      "SSSSSSSSSSSSSSSSSSS\n",
      "inside the reliefF_variable func2\n",
      "7\n",
      "10\n",
      "46\n",
      "1\n",
      "SSSSSSSSSSSSSSSSSSS\n",
      "inside the reliefF_variable func2\n",
      "7\n",
      "10\n",
      "46\n",
      "1\n",
      "SSSSSSSSSSSSSSSSSSS\n",
      "inside the reliefF_variable func2\n",
      "7\n",
      "10\n",
      "46\n",
      "1\n",
      "SSSSSSSSSSSSSSSSSSS\n",
      "inside the reliefF_variable func2\n",
      "variable rfe\n",
      "df_original\n",
      "7\n",
      "7\n",
      "10\n",
      "46\n",
      "1\n",
      "inside the reliefF_variable func\n",
      "7\n",
      "10\n",
      "46\n",
      "1\n",
      "inside the reliefF_variable func\n",
      "7\n",
      "10\n",
      "46\n",
      "1\n",
      "inside the reliefF_variable func\n",
      "7\n",
      "10\n",
      "46\n",
      "1\n",
      "inside the reliefF_variable func\n",
      "7\n",
      "10\n",
      "46\n",
      "1\n",
      "inside the reliefF_variable func\n",
      "7\n",
      "10\n",
      "46\n",
      "1\n",
      "inside the reliefF_variable func\n",
      "7\n",
      "10\n",
      "46\n",
      "1\n",
      "inside the reliefF_variable func\n",
      "7\n",
      "10\n",
      "46\n",
      "1\n",
      "inside the reliefF_variable func\n",
      "7\n",
      "10\n",
      "46\n",
      "1\n",
      "inside the reliefF_variable func\n",
      "7\n",
      "10\n",
      "46\n",
      "1\n",
      "inside the reliefF_variable func\n",
      "variable reliefFTwo\n",
      "cr\n",
      "1\n",
      "Support Vector Classifier  +  Original  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Decision Tree Classifier  +  Original  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Random Forest Classifier  +  Original  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Gaussian Naive Bayes  +  Original  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Gradient Boosting Classifier  +  Original  +  setapProcessT1_updated1.csv \n",
      "\n",
      "K Nearest Neighbour  +  Original  +  setapProcessT1_updated1.csv \n",
      "\n",
      "cr\n",
      "2\n",
      "Support Vector Classifier  +  ReliefF  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Decision Tree Classifier  +  ReliefF  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Random Forest Classifier  +  ReliefF  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Gaussian Naive Bayes  +  ReliefF  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Gradient Boosting Classifier  +  ReliefF  +  setapProcessT1_updated1.csv \n",
      "\n",
      "K Nearest Neighbour  +  ReliefF  +  setapProcessT1_updated1.csv \n",
      "\n",
      "cr\n",
      "3\n",
      "Support Vector Classifier  +  ChiSquare  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Decision Tree Classifier  +  ChiSquare  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Random Forest Classifier  +  ChiSquare  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Gaussian Naive Bayes  +  ChiSquare  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Gradient Boosting Classifier  +  ChiSquare  +  setapProcessT1_updated1.csv \n",
      "\n",
      "K Nearest Neighbour  +  ChiSquare  +  setapProcessT1_updated1.csv \n",
      "\n",
      "cr\n",
      "4\n",
      "Support Vector Classifier  +  SVMRFE  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Decision Tree Classifier  +  SVMRFE  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Random Forest Classifier  +  SVMRFE  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Gaussian Naive Bayes  +  SVMRFE  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Gradient Boosting Classifier  +  SVMRFE  +  setapProcessT1_updated1.csv \n",
      "\n",
      "K Nearest Neighbour  +  SVMRFE  +  setapProcessT1_updated1.csv \n",
      "\n",
      "cr\n",
      "5\n",
      "Support Vector Classifier  +  Variable ReliefF  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Decision Tree Classifier  +  Variable ReliefF  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Random Forest Classifier  +  Variable ReliefF  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Gaussian Naive Bayes  +  Variable ReliefF  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Gradient Boosting Classifier  +  Variable ReliefF  +  setapProcessT1_updated1.csv \n",
      "\n",
      "K Nearest Neighbour  +  Variable ReliefF  +  setapProcessT1_updated1.csv \n",
      "\n",
      "cr\n",
      "6\n",
      "Support Vector Classifier  +  ReliefFTwo  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Decision Tree Classifier  +  ReliefFTwo  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Random Forest Classifier  +  ReliefFTwo  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Gaussian Naive Bayes  +  ReliefFTwo  +  setapProcessT1_updated1.csv \n",
      "\n",
      "Gradient Boosting Classifier  +  ReliefFTwo  +  setapProcessT1_updated1.csv \n",
      "\n",
      "K Nearest Neighbour  +  ReliefFTwo  +  setapProcessT1_updated1.csv \n",
      "\n",
      "df shapes\n",
      "(74, 85)\n",
      "chi square\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1614], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path_name \u001b[38;5;129;01min\u001b[39;00m path_name_list:\n\u001b[1;32m---> 16\u001b[0m   main_function(path_name, df_scores,feature_names_path, count)\n",
      "Cell \u001b[1;32mIn[1613], line 47\u001b[0m, in \u001b[0;36mmain_function\u001b[1;34m(path_name, df_scores, feature_names_path, count)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchi square\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m#ReliefF\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m reliefF_features \u001b[38;5;241m=\u001b[39m reliefF(df, number_of_neighbours, instances_to_select, number_of_features)\n\u001b[0;32m     48\u001b[0m df_reliefF \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path_name, usecols \u001b[38;5;241m=\u001b[39m reliefF_features)\n\u001b[0;32m     49\u001b[0m df_reliefF \u001b[38;5;241m=\u001b[39m normalize_dataframe(df_reliefF)\n",
      "Cell \u001b[1;32mIn[1593], line 88\u001b[0m, in \u001b[0;36mreliefF\u001b[1;34m(df, number_of_neighbours, instances_to_select, number_of_features)\u001b[0m\n\u001b[0;32m     85\u001b[0m postProb\u001b[38;5;241m=\u001b[39mpclass\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mprobability_random_instance_label) \u001b[38;5;66;03m#calculating the posterior probability of getting this class\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m each_miss \u001b[38;5;129;01min\u001b[39;00m nearest_misses[each_label]:\n\u001b[1;32m---> 88\u001b[0m   mI \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39miloc[each_miss,:]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     89\u001b[0m   dRM \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdivide(np\u001b[38;5;241m.\u001b[39mabs(np\u001b[38;5;241m.\u001b[39msubtract(random_instance_features,mI)),difference)\n\u001b[0;32m     90\u001b[0m   dRM \u001b[38;5;241m=\u001b[39m dRM\u001b[38;5;241m/\u001b[39m(instances_to_select \u001b[38;5;241m*\u001b[39m number_of_neighbours)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1147\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[1;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple(key)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1150\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1654\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1652\u001b[0m tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tuple_indexer(tup)\n\u001b[0;32m   1653\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[1;32m-> 1654\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_lowerdim(tup)\n\u001b[0;32m   1656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple_same_dim(tup)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1039\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tup):\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_label_like(key):\n\u001b[0;32m   1037\u001b[0m         \u001b[38;5;66;03m# We don't need to check for tuples here because those are\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m         \u001b[38;5;66;03m#  caught by the _is_nested_tuple_indexer check above.\u001b[39;00m\n\u001b[1;32m-> 1039\u001b[0m         section \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_axis(key, axis\u001b[38;5;241m=\u001b[39mi)\n\u001b[0;32m   1041\u001b[0m         \u001b[38;5;66;03m# We should never have a scalar section here, because\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m         \u001b[38;5;66;03m#  _getitem_lowerdim is only called after a check for\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m         \u001b[38;5;66;03m#  is_scalar_access, which that would be.\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m section\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim:\n\u001b[0;32m   1045\u001b[0m             \u001b[38;5;66;03m# we're in the middle of slicing through a MultiIndex\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m             \u001b[38;5;66;03m# revise the key wrt to `section` by inserting an _NS\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1716\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1713\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m   1714\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_integer(key, axis)\n\u001b[1;32m-> 1716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3789\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[1;34m(self, i, axis)\u001b[0m\n\u001b[0;32m   3787\u001b[0m \u001b[38;5;66;03m# irow\u001b[39;00m\n\u001b[0;32m   3788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 3789\u001b[0m     new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mfast_xs(i)\n\u001b[0;32m   3791\u001b[0m     \u001b[38;5;66;03m# if we are a copy, mark as such\u001b[39;00m\n\u001b[0;32m   3792\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(new_mgr\u001b[38;5;241m.\u001b[39marray, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m new_mgr\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mbase \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:980\u001b[0m, in \u001b[0;36mBlockManager.fast_xs\u001b[1;34m(self, loc)\u001b[0m\n\u001b[0;32m    975\u001b[0m     result \u001b[38;5;241m=\u001b[39m ensure_wrapped_if_datetimelike(result)\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m    978\u001b[0m     \u001b[38;5;66;03m# Such assignment may incorrectly coerce NaT to None\u001b[39;00m\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;66;03m# result[blk.mgr_locs] = blk._slice((slice(None), loc))\u001b[39;00m\n\u001b[1;32m--> 980\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, rl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(blk\u001b[38;5;241m.\u001b[39mmgr_locs):\n\u001b[0;32m    981\u001b[0m         result[rl] \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39miget((i, loc))\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# path_name_list = ['/content/gastroenterology.csv','/content/leukemia.csv', '/content/colon 2000.csv', '/content/DLBCL.csv', '/content/LSVT_voice_rehabilitation.csv', '/content/gastric cancer.csv',]\n",
    "#path_name_list = ['/content/staDynBenignLab.csv','/content/qsar_androgen_receptor.csv','/content/qsar_oral_toxicity.csv']\n",
    "# path_name_list = ['setapProcessT1_updated1.csv']\n",
    "# path_name_list = ['staDynBenignLab.csv','taiwan_bank_data_updated.csv','pd_speech_data_updated.csv','aps_data_updated.csv','swarm_grouped_data_updated.csv','tuandromd_data_updated.csv']\n",
    "# path_name_list = ['leukemia.csv','colon 2000.csv']\n",
    "\n",
    "path_name_list = ['setapProcessT1_updated1.csv','setapProcessT2_updated.csv','setapProcessT3_updated.csv','setapProcessT4_updated.csv','setapProcessT5_updated.csv','setapProcessT6_updated.csv','setapProcessT7_updated.csv','setapProcessT8_updated.csv','setapProcessT9_updated.csv','setapProcessT10_updated.csv','setapProcessT11_updated.csv','period_data_updated.csv','darwin_data_updated.csv','toxicity_data_updated.csv','voice_data_updated.csv','colon 2000.csv','DLBCL.csv','gastric cancer.csv','gastroenterology.csv','leukemia.csv']\n",
    "# path_name_list = ['gastric cancer.csv']\n",
    "# path_name_list = ['setapProcessT6_updated.csv']\n",
    "# path_name_list = ['colon 2000.csv','DLBCL.csv','gastric cancer.csv','gastroenterology.csv','leukemia.csv']\n",
    "df_scores = pd.DataFrame(columns=['Algorithm','Dataset','Model','Fold Number', 'Accuracy','F1','Geometric Mean','AUC','Precision','Recall'])\n",
    "# Add Precision and recall \n",
    "feature_names_path = 'featuresLow.csv'\n",
    "count = 1\n",
    "for path_name in path_name_list:\n",
    "  main_function(path_name, df_scores,feature_names_path, count)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdibIG27sLNn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_scores.to_csv('scores_main_combinations.csv')\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print(\"Total execution time:\", end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ml7dcKzqzPlX"
   },
   "outputs": [],
   "source": [
    "# #path_name_list = ['/content/gastroenterology.csv','/content/leukemia.csv', '/content/colon 2000.csv', '/content/DLBCL.csv', '/content/LSVT_voice_rehabilitation.csv', '/content/gastric cancer.csv',]\n",
    "# # path_name_list = ['/content/staDynBenignLab.csv','/content/qsar_androgen_receptor.csv','/content/qsar_oral_toxicity.csv']\n",
    "# path_name_list = ['gastroenterology.csv']\n",
    "# df_scores = pd.DataFrame(columns=['Algorithm','Dataset','Model','Fold Number', 'Accuracy','F1','Geometric Mean','AUC'])\n",
    "# feature_names_path = 'featuresHigh.csv'\n",
    "# count = 1\n",
    "# for path_name in path_name_list:\n",
    "#   main_function(path_name, df_scores, feature_names_path, count)\n",
    "#   count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1MxGg60zQpt"
   },
   "outputs": [],
   "source": [
    "# df_scores.to_csv('h1.csv')\n",
    "# df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMn3ok1eurk1hYFlrx88QjX",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Feature Selection.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
